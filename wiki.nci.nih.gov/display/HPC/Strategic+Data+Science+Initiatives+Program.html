Overview
<h2 id="StrategicDataScienceInitiativesProgram-Reliable,innovativesolutionsthatmakeiteasyforNCIinvestigatorstousehighperformancecomputing.">
  Reliable, innovative solutions that make it easy for NCI investigators to use high performance computing.
</h2>
<h2 id="StrategicDataScienceInitiativesProgram-ForinformationonhowtoaccessDepartmentofEnergy(DOE)HPCSystems,clickononeofthefollowingDOELabicons:">
  For information on how to access Department of Energy (DOE) HPC Systems, click on one of the following DOE Lab icons:
</h2>
<img alt="Argonne National Laboratory logo." width="250" src="https://wiki.nci.nih.gov/download/attachments/embedded-page/HPC/Strategic%20Data%20Science%20Initiatives%20Program/ANL%20Logo.JPG?api=v2">
<img alt="Oak Ridge National Laboratory logo." width="250" src="https://wiki.nci.nih.gov/download/attachments/embedded-page/HPC/Strategic%20Data%20Science%20Initiatives%20Program/ORNL%20Logo.JPG?api=v2">
<ul></ul>
....
<a href="/display/HPC/HPC+Consultation" rel="nofollow">
  search

  HPC Consultation
</a>
- Consultation, training, and support for high performance computing systems used by NCI researchers.
<a href="/display/HPC/HPC+Application+Development" rel="nofollow">
  search

  HPC Application Development
</a>
- High performance (HPC) and scientific computing application development for NCI researchers.
<a href="/display/HPC/HPC+Data+Transfer+Resources" rel="nofollow">
  search

  HPC Data Transfer Resources
</a>
- High performance and scientific computing data transfer resources for NCI cancer research studies/projects.




....Biomedical research computation enables a growing number of scientists and clinicians to analyze, use, manage, and share their data—and their discoveries.At NCI, we have developed innovative High Performance (HPC) and Scientific Computing services that help investigators accelerate and advance their cancer research.Our services include HPC support and consultation, education, and reliable access to various state-of-the-art HPC systems including the
<a href="https://hpc.nih.gov/docs/accounts.html" rel="nofollow">
  NIH Biowulf system
</a>
, the
<a href="https://frederick.cancer.gov/about/default.aspx" rel="nofollow">
  NCI MOAB system
</a>
, and
<a href="https://energy.gov/science-innovation/science-technology/computing" rel="nofollow">
  HPC programs at the Department of Energy
</a>
.  We also work closely with investigators across NCI to develop, optimize and/or validate HPC applications—and computing technologies—that enable rapid data transfer and provide effective data management. With ongoing input from the cancer researchers we serve, we continue to improve and evolve our HPC services, capabilities and opportunities. As partners to cancer researchers worldwide, we are guided by investigators’ challenges and opportunities.Above all, we seek to expand scientists’ use of high performance computing to accelerate discoveries in predictive oncology research and advance effective methods of prevention, treatment, and cure. Context....CBIIT HPC StrategyWith a focus on providing robust and reliable solutions enabling NCI investigators to utilize HPC in their efforts, the CBIIT HPC strategy is focused on these interconnected areas:
<ul>
  <li>
    Working closely with investigators across NCI to enable broader utilization of HPC through HPC training, education and reliable system access
  </li>
  <li>
    Guided by investigator challenges and opportunities, provide support and consulting for HPC needs as well as development, optimization and/or validation of HPC applications useful to cancer research and clinical applications
  </li>
  <li>
    Effective data management and information delivery solutions in support of HPC applications used in cancer research
  </li>
  <li>
    Exploration and evaluation of emerging HPC technologies for use in cancer research, information delivery and data center operations
  </li>
  <li>
    Developing essential partnerships within NCI, NIH, HHS, government, academically, commercially, nationally and internationally fostering the expanded use of HPC in cancer research
  </li>
  <li>
    Develop and deliver services supporting current and future HPC needs of NCI investigators
  </li>
  <li>
    Continual incorporation of investigator input to improve and evolve HPC services, capabilities and opportunities
  </li>
</ul>
Long Range Guiding Objectives for HPC in Cancer ResearchWith guidance and insight provided by the cancer research and clinical community within NCI, deliver robust, reliable HPC capabilities and support that:
<ul>
  <li>
    Enable broader understanding of cancer, cancer system dynamics and cancer characterizations
  </li>
  <li>
    Enable rapid identification of potential cancer risks and presence of cancer in individuals
  </li>
  <li>
    Enable rapid determination of optimal treatment options for patients
  </li>
  <li>
    Expand treatment options through improved discovery and rapid, reliable validation
  </li>
  <li>
    Foster computational integration and cooperation across the global cancer research community
  </li>
  <li>
    Enable transfer and flow of HPC technologies between NCI and other stakeholders
  </li>
  <li>
    Enable NCI to take full advantage of computational advances to accelerate cancer research
  </li>
</ul>
Foundations for Successful HPC in Cancer Research and Clinical Development
<ul>
  <li>
    Useful
    <ul>
      <li>
        Performing needed functions and delivering key capabilities
      </li>
      <li>
        Enabling technologies can be eventually used in clinical application
      </li>
    </ul>
  </li>
  <li>
    Reliable
    <ul>
      <li>
        Assuring new computing technologies and applications are functionally reliable
      </li>
      <li>
        Assuring technologies and applications are validated and verified
      </li>
      <li>
        Assuring appropriate reproducibility of delivered solutions over time
      </li>
    </ul>
  </li>
  <li>
    Adaptive
    <ul>
      <li>
        Exploring new and emerging technologies and applications for use in cancer research and translation
      </li>
      <li>
        Utilizing multiple sources of input (internal and external) to improve overall HPC capabilities
      </li>
    </ul>
  </li>
  <li>
    Portable
    <ul>
      <li>
        Enabling intellectual investments to transition across emerging and evolving technology platforms
      </li>
    </ul>
  </li>
  <li>
    Efficient
    <ul>
      <li>
        Providing solutions in as rapid as possible yet in a cost-effective manner
      </li>
    </ul>
  </li>
</ul>
....
<a href="/display/HPC/HPC+Thought+Leaders+Presentations" rel="nofollow">
  search

  HPC Thought Leaders Group
</a>
<a href="/display/HPC/CBIIT+TechScouts" rel="nofollow">
  search

  CBIIT TechScouts
</a>
HPC Presentation to NCI Informatics &amp; IT Advisory Group - Eric Stahlberg - 10.20.16




....
<a href="https://wiki.nci.nih.gov/display/HPC/High+Performance+Computing+Special+Interest+Group" rel="nofollow">
  CLICK HERE
</a>
to access the HPC Special Interest Group Training and Education site.Training and Education Archive
<a href="https://wiki.nci.nih.gov/download/attachments/341187581/Globus%20User-Focused%20Webinar-20170516%201600-1%20%281%29.arf?version=1&amp;modificationDate=1495033132380&amp;api=v2" rel="nofollow">
  Globus User-Focused Webinar
</a>
 - Click to view the Globus NIH User-focused Webinar (48 mins) held on May 16, 2017HPC in Biomedical and Life Science Applications - Click below to view presentation
<a href="/display/HPC/Strategic+Data+Science+Initiatives+Program?preview=%2F324044894%2F349340589%2FHPC++in+Biomedical+and+Life+Science+Applications+updated.pptx" rel="nofollow">
  HPC  in Biomedical and L…
</a>
Click here for description.......
<ul>
  <li>
    High Performance Computing (HPC) has an invaluable impact on driving advances in cancer research. Projects conducted within the context of the Precision Medicine Initiative, the National Cancer Institute, the Department of Energy pilots, and the National Strategic Computing Initiative are only examples of how new frontiers can be pushed using the next generation of supercomputers.
  </li>
  <li>
    Areas of impact include, but are not limited to:
    <ul>
      <li>
        predicative algorithms for cancer therapy using machine learning,
      </li>
      <li>
        molecular dynamics simulation modeling for drug discovery, and
      </li>
      <li>
        medical image analysis.
      </li>
    </ul>
  </li>
  <li>
    This presentation highlights different programming models and run times that expose parallelism on multicore/multiprocessors platforms such as shared and distributed memory architectures, and Graphics Processing Units (GPUs). Target audience are computational and data scientist looking to do more with available HPC resources and prepare for capabilities of even larger systems than those currently available.
  </li>
  <li>
    An overview on the projects conducted at the NCI is given. This includes the fields of molecular dynamics simulation, analysis of transcript slicing, and image segmentation and analysis.  Some best practices and lessons learned will also be shared.
  </li>
</ul>
Questions?

General Support : Miles Kimbrough |
<a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
  miles.kimbrough@nih.gov
</a>
 | 240.276.5251Consultation and Guidance : Eric Stahlberg |
<a href="mailto:eric.stahlberg@nih.gov" rel="nofollow">
  eric.stahlberg@nih.gov
</a>
 | 240.276.6729Technical Support : George Zaki |
<a href="mailto:george.zaki@nih.gov" rel="nofollow">
  george.zaki@nih.gov
</a>
 | 240.276.5171







Monthly Updates
Click Here for Recent UpdatesTable of Contents
<ul>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-7/12/18" rel="nofollow">
      7/12/18
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-6/11/18" rel="nofollow">
      6/11/18
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-4/2/18" rel="nofollow">
      4/2/18
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-3/2/18" rel="nofollow">
      3/2/18
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-2/2/18" rel="nofollow">
      2/2/18
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-1/9/18" rel="nofollow">
      1/9/18
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-9/3/17" rel="nofollow">
      9/3/17
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-8/5/17" rel="nofollow">
      8/5/17
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-7/10/17" rel="nofollow">
      7/10/17
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-6/6/17" rel="nofollow">
      6/6/17
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-5/11/17" rel="nofollow">
      5/11/17
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-4/11/17" rel="nofollow">
      4/11/17
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-3/6/17" rel="nofollow">
      3/6/17
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-2/6/17" rel="nofollow">
      2/6/17
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-1/10/17" rel="nofollow">
      1/10/17
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-12/16/16" rel="nofollow">
      12/16/16
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-12/8/16" rel="nofollow">
      12/8/16
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-10/3/16" rel="nofollow">
      10/3/16
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-9/6/16" rel="nofollow">
      9/6/16
    </a>
  </li>
  <li>
    <a href="#StrategicDataScienceInitiativesProgram-8/8/16" rel="nofollow">
      8/8/16
    </a>
    <ul>
      <li>
        Blog Posts
      </li>
    </ul>
  </li>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-7/12/18">
  7/12/18
</h3>
<ul>
  <li>
    A
    <a href="mailto:NciDataVault@nih.gov" rel="nofollow">
      new mailbox
    </a>
    for NCI Data Vault Services is now available for staff in your DOCs. NCI Data Vault Services can be leveraged to store, manage, share, and transfer data. This service provides a one-stop solution for all your data archival needs.
  </li>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-6/11/18">
  6/11/18
</h3>
<ul>
  <li>
    In collaboration with the Department of Energy National Labs, the
    <a href="https://ncip.nci.nih.gov/blog/candle-scaling-jdacs4c-algorithms-unprecedented-magnitudes/?utm_campaign=informaticsandITnewsletter&amp;utm_medium=email&amp;utm_source=informaticsandIT_5%2F9%2F18" rel="nofollow">
      CANcer Distributed Learning Environment
    </a>
     (CANDLE) is currently available on
    <a href="https://hpc.nih.gov/docs/userguide.html?utm_campaign=informaticsandITnewsletter&amp;utm_medium=email&amp;utm_source=informaticsandIT_5%2F9%2F18" rel="nofollow">
      Biowulf
    </a>
    to optimize and train deep neural networks at scale. Please contact
    <a href="mailto:george.zaki@nih.gov" rel="nofollow">
      George Zaki
    </a>
    if you would like to know more information about this technology.
  </li>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-4/2/18">
  4/2/18
</h3>
<ul>
  <li>
    Upgrades to the High Performance Computing and Data Management Environment (HPC DME) are underway. These include several updates to the existing environment to improve overall usability and productivity. For more information about using the HPCDME API to archive and annotate your data, contact
    <a href="mailto:george.zaki@nih.gov" rel="nofollow">
      George Zaki
    </a>
     or
    <a href="mailto:zhengwu.lu@nih.gov" rel="nofollow">
      Zhengwu Lu
    </a>
    .
  </li>
  <li>
    Join the
    <a href="https://wiki.nci.nih.gov/display/HPC/High+Performance+Computing+Special+Interest+Group?utm_campaign=informaticsandITnewsletter&amp;utm_medium=email&amp;utm_name=informaticsandITnews&amp;utm_source=informaticsandIT_3%2F8%2F18" rel="nofollow">
      HPC Special Interest Group
    </a>
    ! The group meets regularly to discuss needs and opportunities for using high-performance and large scale computing to accelerate research insights. If you are interested in becoming part of this group and join upcoming meetings, please contact
    <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
      Miles Kimbrough
    </a>
    or
    <a href="mailto:randall.johnson@nih.gov" rel="nofollow">
      Randy Johnson
    </a>
    .
  </li>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-3/2/18">
  3/2/18
</h3>
<ul>
  <li>
    Due to increasing demand, NIH will be planning a subsequent Cancer Deep Learning Workshop in the coming months to accommodate those unable to attend the
    <a href="https://wiki.nci.nih.gov/pages/viewpage.action?pageId=357701616&amp;utm_campaign=informaticsandITnewsletter&amp;utm_medium=email&amp;utm_source=informaticsandIT_3%2F8%2F18" rel="nofollow">
      February CANDLE workshop
    </a>
    or those newly interested in learning about deep and machine learning applications in advancing scientific research. Contact
    <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
      Miles Kimbrough
    </a>
     or
    <a href="mailto:george.zaki@nih.gov" rel="nofollow">
      George Zaki
    </a>
     to stay informed as this workshop develops.
  </li>
  <li>
    Upgrades to the High Performance Computing and Data Management Environment (HPC DME) are underway. These include several updates to the existing environment to improve overall usability and productivity. For more information about using the HPCDME API to archive and annotate your data, contact
    <a href="mailto:george.zaki@nih.gov" rel="nofollow">
      George Zaki
    </a>
     or
    <a href="mailto:zhengwu.lu@nih.gov" rel="nofollow">
      Zhengwu Lu
    </a>
    .
  </li>
  <li>
    Join the
    <a href="https://wiki.nci.nih.gov/display/HPC/High+Performance+Computing+Special+Interest+Group?utm_campaign=informaticsandITnewsletter&amp;utm_medium=email&amp;utm_name=informaticsandITnews&amp;utm_source=informaticsandIT_3%2F8%2F18" rel="nofollow">
      HPC Special Interest Group
    </a>
    ! The group meets regularly to discuss needs and opportunities for using high-performance and large scale computing to accelerate research insights. If you are interested in becoming part of this group and join upcoming meetings, please contact
    <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
      Miles Kimbrough
    </a>
    or
    <a href="mailto:randall.johnson@nih.gov" rel="nofollow">
      Randy Johnson
    </a>
    .
  </li>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-2/2/18">
  2/2/18
</h3>
<ul>
  <li>
    Although maximum capacity has been reached for physical attendance, NIH will be virtually hosting the
    <a href="https://wiki.nci.nih.gov/display/HPC/CANDLE+Deep+Learning+Workshop+@+NIH" rel="nofollow">
      Cancer Deep Learning Workshop
    </a>
    on February 21-23 via WebEx. The
    <a href="https://cbiit.cancer.gov/ncip/hpc/candle?utm_medium=newsletter&amp;utm_name=informaticsandITnews&amp;utm_source=newsletter+December+2017" rel="nofollow">
      CANcer Distributed Learning Environment (CANDLE)
    </a>
     is designed to use machine-learning algorithms to find patterns in large datasets with the goal of offering insights that may ultimately result in improved cancer treatment. The workshop will provide participants the opportunity to learn more about deep learning, share insights into research, and talk about the future potential of the CANcer Distributed Learning Environment through open-ended discussions and hands-on activities. Contact
    <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
      Miles Kimbrough
    </a>
     or
    <a href="mailto:george.zaki@nih.gov" rel="nofollow">
      George Zaki
    </a>
     to register.
  </li>
  <li>
    Due to increasing demand, the NIH will be planning a subsequent Cancer Deep Learning Workshop in the coming months to accommodate those who were either unable to attend the February workshop or newly interested in learning about deep and machine learning applications in advancing scientific research. Contact
    <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
      Miles Kimbrough
    </a>
     or
    <a href="mailto:george.zaki@nih.gov" rel="nofollow">
      George Zaki
    </a>
     to keep informed as this workshop develops.
  </li>
  <li>
    The High Performance Computing and Data Management Environment (HPC DME) API v1.6.0 was released January 30, 2018, and includes several API, Web UI, Client Utility improvements and bug fixes to the existing environment. For more information about using the HPCDME API to archive and annotate your data, contact
    <a href="mailto:george.zaki@nih.gov" rel="nofollow">
      George Zaki
    </a>
    or
    <a href="mailto:zhengwu.lu@nih.gov" rel="nofollow">
      Zhengwu Lu
    </a>
    .
  </li>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-1/9/18">
  1/9/18
</h3>
<ul>
  <li>
    The
    <a href="https://wiki.nci.nih.gov/display/HPC/High+Performance+Computing+Special+Interest+Group?utm_medium=newsletter&amp;utm_name=informaticsandITnews&amp;utm_source=newsletter+December+2017" rel="nofollow">
      HPC Special Interest Group (HPC SIG)
    </a>
    , a collaborative effort between the NCI Data Science and Information Technology Program (DSITP) and CBIIT, will host a brief tutorial on using containers on Biowulf. The session will be held on Wednesday, January 17 from 9:00 a.m. to 10:00 a.m. at the Shady Grove campus in Room 6W032/034. It will cover containers and how to utilize technologies such as Singularity on Biowulf. For more information and registration instructions, contact
    <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
      Miles Kimbrough
    </a>
     or
    <a href="mailto:george.zaki@nih.gov" rel="nofollow">
      George Zaki
    </a>
    .
  </li>
  <li>
    NIH will host a
    <a href="https://wiki.nci.nih.gov/display/HPC/CANDLE+Deep+Learning+Workshop+@+NIH" rel="nofollow">
      Cancer Deep Learning Workshop
    </a>
    on February 21-23 in Building 35A. The
    <a href="https://cbiit.cancer.gov/ncip/hpc/candle?utm_medium=newsletter&amp;utm_name=informaticsandITnews&amp;utm_source=newsletter+December+2017" rel="nofollow">
      CANcer Distributed Learning Environment (CANDLE)
    </a>
     is designed to use machine-learning algorithms to find patterns in large datasets with the goal of offering insights that may ultimately result in improved cancer treatment. The workshop will provide participants the opportunity to learn more about deep learning, share insights into research, and talk about the future potential of the CANcer Distributed Learning Environment through open-ended discussions and hands-on activities. Contact
    <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
      Miles Kimbrough
    </a>
     or
    <a href="mailto:george.zaki@nih.gov" rel="nofollow">
      George Zaki
    </a>
     to register.
  </li>
  <li>
    The High Performance Computing and Data Management Environment (HPC DME) API has new higher bandwidth transfers between the Biowulf system and Cleversafe object store technology located in Frederick. Using new presigned S3 URL technologies, the multithreaded implementation has demonstrated over 2 Gbps (gigabits per second) transfer speed from end to end for large files. For more information about using the HPCDME API to archive and annotate your data, contact
    <a href="mailto:george.zaki@nih.gov" rel="nofollow">
      George Zaki
    </a>
    or
    <a href="mailto:zhengwu.lu@nih.gov" rel="nofollow">
      Zhengwu Lu
    </a>
    .
  </li>
</ul>
10/6/17
<ul>
  <li>
    HPC Special Interest Group – Initiated through collaborative efforts between the NCI Data Science and Information Technology Program (DSITP) and CBIIT, the HPC Special Interest Group (HPC SIG) aims to build a community around high performance computing with the shared purpose of raising scientific productivity.
  </li>
  <ul>
    <li>
      We are looking for information from you on how you are using or would like to use large scale computing for cancer research.
    </li>
    <li>
      Information and support may be facilitated through a general needs assessment questionnaire found
      <a href="https://wiki.nci.nih.gov/display/HPC/High+Performance+Computing+Special+Interest+Group?preview=/348327437/348327444/HPC%20Needs%20Assessment%20Template%20May%202017.docx" rel="nofollow">
        HERE
      </a>
      , by submitting a ticket
      <a href="https://service.cancer.gov/nav_to.do?uri=%2Fcom.glideapp.servicecatalog_cat_item_view.do%3Fv%3D1%26sysparm_id%3D46bc4d6f6fa0d200e04fd15eae3ee43d%26sysparm_catalog%3De0d08b13c3330100c8b837659bba8fb4" rel="nofollow">
        HERE
      </a>
      , or by contacting
      <a href="mailto:eric.stahlberg@nih.gov" rel="nofollow">
        Eric Stahlberg
      </a>
      or
      <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
        Miles Kimbrough
      </a>
      .
    </li>
  </ul>
  <li>
    Computational Approaches for Cancer Workshop in November – This workshop is a part of the annual
    <a href="http://sc17.supercomputing.org/" rel="nofollow">
      Supercomputing Conference
    </a>
    which brings together world leaders in IT,  storage, computing, networking and analytics used in scientific computing applications including cancer.  This year’s program will include a special session on Machine Learning Applied to Cancer – highlighting advances and paper submissions from the HPC and cancer research communities.
  </li>
  <ul>
    <li>
      The workshop will be held Friday, November 17, in Denver, CO.
    </li>
    <li>
      Follow the link
      <a href="http://www.scworkshops.net/cancer2017" rel="nofollow">
        HERE
      </a>
       for more information or reach out to
      <a href="mailto:eric.stahlberg@nih.gov" rel="nofollow">
        Eric Stahlberg
      </a>
       for more details.
    </li>
  </ul>
</ul>
<ul>
  <li>
    HPC Data Management Support – The HPC data management environment (DME) provides a number of application programming interfaces (APIs) to support common scientific data/metadata management functions across the NCI.  HPC DME archive storage can be a permanent storage solution for user data, and can be used as a platform to search, manage and transfer data onto other storage systems.
  </li>
  <ul>
    <li>
      To obtain an HPC DME account, please contact the DME team
      <a href="mailto:HPC_DME_Admin@nih.gov" rel="nofollow">
        HERE
      </a>
      .
    </li>
    <li>
      For any other questions, please contact either
      <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
        Miles Kimbrough
      </a>
      or
      <a href="mailto:george.zaki@nih.gov" rel="nofollow">
        George Zaki
      </a>
      .
    </li>
  </ul>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-9/3/17">
  9/3/17
</h3>
<ul>
  <li>
    HPC Special Interest Group – Initiated through collaborative efforts between the NCI Data Science and Information Technology Program (DSITP) and CBIIT, the HPC Special Interest Group (HPC SIG) aims to build a community around high performance computing with the shared purpose of raising scientific productivity.
  </li>
  <ul>
    <li>
      We are looking for information from you on how you are using or would like to use large scale computing for cancer research.
    </li>
    <li>
      Information and support may be facilitated through a general needs assessment questionnaire found
      <a href="https://wiki.nci.nih.gov/display/HPC/High+Performance+Computing+Special+Interest+Group?preview=/348327437/348327444/HPC%20Needs%20Assessment%20Template%20May%202017.docx" rel="nofollow">
        HERE
      </a>
      , by submitting a ticket
      <a href="https://service.cancer.gov/nav_to.do?uri=%2Fcom.glideapp.servicecatalog_cat_item_view.do%3Fv%3D1%26sysparm_id%3D46bc4d6f6fa0d200e04fd15eae3ee43d%26sysparm_catalog%3De0d08b13c3330100c8b837659bba8fb4" rel="nofollow">
        HERE
      </a>
      , or by contacting
      <a href="mailto:eric.stahlberg@nih.gov" rel="nofollow">
        Eric Stahlberg
      </a>
      or
      <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
        Miles Kimbrough
      </a>
      .
    </li>
  </ul>
  <li>
    Computational Approaches for Cancer Workshop in November – This workshop is a part of the annual
    <a href="http://sc17.supercomputing.org/" rel="nofollow">
      Supercomputing Conference
    </a>
    which brings together world leaders in IT,  storage, computing, networking and analytics used in scientific computing applications including cancer.  This year’s program will include a special session on Machine Learning Applied to Cancer – highlighting advances and paper submissions from the HPC and cancer research communities.
  </li>
  <ul>
    <li>
      The workshop will be held Friday, November 17, in Denver, CO.
    </li>
    <li>
      Consult
      <a href="http://www.scworkshops.net/cancer2017" rel="nofollow">
        http://www.scworkshops.net/cancer2017
      </a>
       or reach out to
      <a href="mailto:eric.stahlberg@nih.gov" rel="nofollow">
        Eric Stahlberg
      </a>
       for more details.
    </li>
  </ul>
</ul>
<ul>
  <li>
    HPC Data Management Support – The HPC data management environment (DME) provides a number of application programming interfaces (APIs) to support common scientific data/metadata management functions across the NCI.  HPC DME archive storage can be a permanent storage solution for user data, and can be used as a platform to search, manage and transfer data onto other storage systems.
  </li>
  <ul>
    <li>
      To obtain an HPC DME account, please contact the DME team at
      <a href="mailto:HPC_DME_Admin@nih.gov" rel="nofollow">
        HPC_DME_Admin@nih.gov
      </a>
      .
    </li>
    <li>
      For any other questions, please contact either Miles Kimbrough (
      <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
        miles.kimbrough@nih.gov
      </a>
      ) or George Zaki (
      <a href="mailto:George.zaki@nih.gov" rel="nofollow">
        George.zaki@nih.gov
      </a>
      )
    </li>
  </ul>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-8/5/17">
  8/5/17
</h3>
<ul>
  <li>
    HPC Special Interest Group – Initiated through collaborative efforts between the NCI Data Science and Information Technology Program (DSITP) and CBIIT, the HPC Special Interest Group (HPC SIG) aims to build a community around high performance computing with the shared purpose of raising scientific productivity.
  </li>
  <li>
    A second user group meeting was held on 7/25 to discuss available HPC resources to the NCI along with upcoming training and education opportunities.
  </li>
  <li>
    We are looking for information from you on how you are using or would like to use large scale computing for cancer research.
  </li>
  <li>
    Information may be submitted through a general needs assessment questionnaire found
    <a href="https://wiki.nci.nih.gov/display/HPC/High+Performance+Computing+Special+Interest+Group?preview=/348327437/348327444/HPC%20Needs%20Assessment%20Template%20May%202017.docx" rel="nofollow">
      HERE
    </a>
    , by submitting a ticket
    <a href="https://service.cancer.gov/nav_to.do?uri=%2Fcom.glideapp.servicecatalog_cat_item_view.do%3Fv%3D1%26sysparm_id%3D46bc4d6f6fa0d200e04fd15eae3ee43d%26sysparm_catalog%3De0d08b13c3330100c8b837659bba8fb4" rel="nofollow">
      HERE
    </a>
    , or by contacting
    <a href="mailto:eric.stahlberg@nih.gov" rel="nofollow">
      Eric Stahlberg
    </a>
    or
    <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
      Miles Kimbrough
    </a>
    .
  </li>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-7/10/17">
  7/10/17
</h3>
<ul>
  <li>
    HPC Special Interest Group – Initiated through collaborative efforts between the NCI Data Science and Information Technology Program (DSITP) and CBIIT, the HPC Special Interest Group (HPC SIG) aims to build a community around high performance computing with the shared purpose of raising scientific productivity.
  </li>
  <li>
    Resources and information on upcoming events may be found
    <a href="https://wiki.nci.nih.gov/display/HPC/High+Performance+Computing+Special+Interest+Group" rel="nofollow">
      HERE
    </a>
    .
  </li>
  <li>
    Please contact
    <a href="mailto:eric.stahlberg@nih.gov" rel="nofollow">
      Eric Stahlberg
    </a>
    or
    <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
      Miles Kimbrough
    </a>
    to discuss further.
  </li>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-6/6/17">
  6/6/17
</h3>
<ul>
  <li>
    HPC Best Practices Webinar Series – The
    <a href="http://ideas-productivity.us16.list-manage.com/track/click?u=5438ff2caf2456f6ec49ebfbf&amp;id=6812f45a8f&amp;e=ef618799a5" rel="nofollow">
      IDEAS Productivity
    </a>
    project, in partnership with several DOE Computing Facilities and the DOE
    <a href="http://ideas-productivity.us16.list-manage.com/track/click?u=5438ff2caf2456f6ec49ebfbf&amp;id=107f85ed45&amp;e=ef618799a5" rel="nofollow">
      Exascale Computing Project
    </a>
    (ECP) is resuming the webinar series on
    <a href="http://ideas-productivity.us16.list-manage2.com/track/click?u=5438ff2caf2456f6ec49ebfbf&amp;id=7275e89b4c&amp;e=ef618799a5" rel="nofollow">
      Best Practices for HPC Software Developers
    </a>
    , which began last year.
  </li>
  <ul>
    <li>
      Webinars will be held monthly on topics in scientific software development and high-performance computing
    </li>
    <li>
      Participation is free and open to the public, but registration will be required for each event
    </li>
    <li>
      <a href="https://exascaleproject.org/event/python-in-hpc-2/" rel="nofollow">
        Register now
      </a>
      for the next webinar: Python in HPC, occurring Wednesday, June 7, from 1-2 PM
    </li>
    <li>
      Please contact
      <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
        Miles Kimbrough
      </a>
       for more information
    </li>
  </ul>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-5/11/17">
  5/11/17
</h3>
<ul>
  <li>
    Globus user-focused Webinar –
    <a href="mailto:https://www.globus.org/" rel="nofollow">
      Globus
    </a>
    , a cloud-authenticated data management and transfer platform, will be hosting a
    <a href="https://cbiit.webex.com/cbiit/j.php?MTID=ma5e31b1244c78c8c8f1a572e320d2090" rel="nofollow">
      user-focused webinar
    </a>
    on Tuesday, May 16th, to benefit those interested in exchanging datasets across a variety of data sources.  The webinar will provide a high-level overview of Globus, steps to start using the service, and common use cases surrounding data sharing and transfer.
  </li>
  <ul>
    <li>
      Please contact
      <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
        Miles Kimbrough
      </a>
      for more information
    </li>
  </ul>
  <li>
    CANDLE Deep Learning Workshop @ NIH – The first CANDLE Workshop at NIH, held April 18-19, 2017, proved to be a tremendous success in weaving the cancer research community into the emerging computational architecture being developed through CANDLE
  </li>
  <ul>
    <li>
      A
      <a href="https://wiki.nci.nih.gov/pages/viewpage.action?pageId=347474364" rel="nofollow">
        summary report
      </a>
      of the workshop is now available, and updates will also be provided at the May 17th CBIIT Sync
    </li>
    <li>
      Please contact
      <a href="mailto:eric.stahlberg@nih.gov" rel="nofollow">
        Eric Stahlberg
      </a>
      ,
      <a href="mailto:george.zaki@nih.gov" rel="nofollow">
        George Zaki
      </a>
      or
      <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
        Miles Kimbrough
      </a>
      to discuss further
    </li>
  </ul>
  <li>
    Computational Approaches for Cancer Workshop in November – The proposal for the Third Workshop on Computational Approaches for Cancer at the annual SC SuperComputing conference in November has been accepted.  This conference brings together world leaders in IT,  storage, computing, networking and analytics used in scientific computing applications including cancer.
  </li>
  <ul>
    <li>
      Consult
      <a href="http://sc17.supercomputing.org/" rel="nofollow">
        sc17.supercomputing.org
      </a>
       or reach out to
      <a href="mailto:eric.stahlberg@nih.gov" rel="nofollow">
        Eric Stahlberg
      </a>
       for more details
    </li>
  </ul>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-4/11/17">
  4/11/17
</h3>
<ul>
  <li>
    Computational Approaches for Cancer Workshop in November (travel requests for November are due soon!) – The proposal for the Third Workshop on Computational Approaches for Cancer at the annual SC SuperComputing conference in November has been accepted.  This conference brings together world leaders in IT,  storage, computing, networking and analytics used in scientific computing applications including cancer.  Consult
    <a href="http://sc17.supercomputing.org/" rel="nofollow">
      sc17.supercomputing.org
    </a>
     or reach out to
    <a href="mailto:eric.stahlberg@nih.gov" rel="nofollow">
      Eric Stahlberg
    </a>
     for more details.
  </li>
  <li>
    High Performance Computing User Group &amp; Office Hours – The HPC Program is developing a user group with the objective of serving those requiring HPC assistance, and expanding the HPC community within the NCI intramural community.  The program will be implementing office hours in Frederick on a recurring basis, to serve as points of interaction among the developing HPC community and to aid those with HPC-specific needs or questions.  Stay tuned for more information – please contact
    <a href="mailto:eric.stahlberg@nih.gov" rel="nofollow">
      Eric Stahlberg(link sends e-mail)
    </a>
     or
    <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
      Miles Kimbrough(link sends e-mail)
    </a>
     to discuss further.
  </li>
  <li>
    CANDLE Workshop – Serving as one of the key computational frameworks to support the NCI DOE Collaboration, the CANcer Distributed Learning Environment is designed to use machine-learning algorithms to find patterns in large datasets with the goal of offering insights that may ultimately result in improved cancer treatment.  Using this computational architecture, participating DOE labs are focused on accelerating methods to identify promising new treatments; deepening understanding of cancer biology; and understanding the impact of new diagnostics, treatments and patient factors in cancer outcomes.
    <ul>
      <li>
        There will be a workshop on April 18-19 at the NIH which will provide an opportunity to learn more about deep learning, share insights into research, and explore the potential through open-ended discussions and hands-on activities.
      </li>
      <li>
        <a href="https://lms.learning.hhs.gov/Saba/Web/Main/goto/GuestOfferingDetails?offeringId=class000000000123712" rel="nofollow">
          Register here for the workshop
        </a>
         – otherwise please contact
        <a href="mailto:eric.stahlberg@nih.gov" rel="nofollow">
          Eric Stahlberg(link sends e-mail)
        </a>
         or
        <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
          Miles Kimbrough(link sends e-mail)
        </a>
         with questions or to discuss further.
      </li>
    </ul>
  </li>
  <li>
    Globus user-focused Webinar – Globus, a cloud-authenticated data management and transfer platform, will be hosting a user-focused webinar on Tuesday, May 9th, to benefit those interested in exchanging datasets across a variety of sources.  The webinar will provide a high-level overview of Globus, steps to start using the service, and common use cases along the following topics:
    <ul>
      <li>
        When, where, and why to use Globus?
      </li>
      <li>
        NIH account specs - distinction from Globus Plus
      </li>
      <li>
        What do system administrators need to set up managed endpoints?
      </li>
      <li>
        Which endpoints are already set up?
      </li>
      <li>
        How to set up Globus on your own desktop
      </li>
      <li>
        How to transfer and share data
      </li>
      <li>
        If sharing with collaborator, what info does collaborator need?  What do you need to give to collaborator?
      </li>
      <li>
        New Globus command line interface, allowing users to script their transfers
      </li>
      <li>
        Encryption, verification, and expected data transfer speeds as compared to other resources (e.g. FTP)
      </li>
    </ul>
  </li>
  <li>
    CBIIT TechScouts – CBIIT TechScouts currently serves the CBIIT community as a forum for promoting continuous improvement through the cross-fertilization of ideas, experiences and recommendations.  In order to promote broader community engagement and increase awareness of this forum, brief information sessions will be scheduled within regular branch meetings to capture and incorporate feedback into the overall TechScouts engagement strategy.
    <ul>
      <li>
        Stay tuned for more information and scheduling details.  Please contact
        <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
          Miles Kimbrough(link sends e-mail)
        </a>
         to discuss further.
      </li>
    </ul>
  </li>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-3/6/17">
  3/6/17
</h3>
<ul>
  <li>
    High Performance Computing User Group &amp; Office Hours – The HPC Program is developing a user group with the objective of serving those requiring HPC assistance, and expanding the HPC community within the NCI intramural community.  The program will be implementing office hours in Frederick on a recurring basis, to serve as points of interaction among the developing HPC community and to aid those with HPC-specific needs or questions.
  </li>
  <ul>
    <li>
      Stay tuned for more information – please contact Eric Stahlberg (
      <a href="mailto:eric.stahlberg@nih.gov" rel="nofollow">
        eric.stahlberg@nih.gov
      </a>
      ) or Miles Kimbrough (
      <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
        miles.kimbrough@nih.gov
      </a>
      ) to discuss further.
    </li>
  </ul>
  <li>
    CANDLE Workshop – Serving as one of the key computational frameworks to support the NCI DOE Collaboration, the CANcer Distributed Learning Environment is designed to use machine-learning algorithms to find patterns in large datasets with the goal of offering insights that may ultimately result in improved cancer treatment.  Using this computational architecture, participating DOE labs are focused on accelerating methods to identify promising new treatments; deepening understanding of cancer biology; and understanding the impact of new diagnostics, treatments and patient factors in cancer outcomes.
  </li>
  <ul>
    <li>
      There will be a workshop on April 18-19 at the NIH which will provide an opportunity to learn more about deep learning, share insights into research, and explore the potential through open-ended discussions and hands-on activities.
    </li>
    <li>
      Stay tuned for more information and registration details – please contact Miles Kimbrough (
      <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
        miles.kimbrough@nih.gov
      </a>
      ) or Eric Stahlberg (
      <a href="mailto:eric.stahlberg@nih.gov" rel="nofollow">
        eric.stahlberg@nih.gov
      </a>
      ) with questions or to discuss further.
    </li>
  </ul>
  <li>
    Globus user-focused Webinar – Globus, a cloud-authenticated data management and transfer platform, will be hosting a user-focused webinar in mid-April to benefit those interested in exchanging datasets across a variety of sources.  The webinar will provide a high-level overview of Globus, steps to start using the service, and common use cases along the following topics:
  </li>
</ul>
<ul>
  <li>
    <ul>
      <li>
        When, where, and why to use Globus?
      </li>
      <li>
        NIH account specs - distinction from Globus Plus
      </li>
      <li>
        What do system administrators need to set up managed endpoints?
      </li>
      <li>
        Which endpoints are already set up?
      </li>
      <li>
        How to set up Globus on your own desktop
      </li>
      <li>
        How to transfer and share data
      </li>
      <li>
        If sharing with collaborator, what info does collaborator need?  What do you need to give to collaborator?
      </li>
      <li>
        New Globus command line interface, allowing users to script their transfers
      </li>
      <li>
        Encryption, verification, and expected data transfer speeds as compared to other resources (e.g. FTP)
      </li>
    </ul>
  </li>
</ul>
<ul>
  <li>
    More details are forthcoming – in the meantime please contact Miles Kimbrough (
    <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
      miles.kimbrough@nih.gov
    </a>
    ) to discuss further.
  </li>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-2/6/17">
  2/6/17
</h3>
<ul>
  <li>
    HPC Data Management Environment 1.0.0 Release – The HPC data management environment (DME) provides a number of application programming interfaces (APIs) to support common scientific data/metadata management functions across the NCI.  Currently in initial release,  HPC DME archive storage can be a permanent storage solution for user data, and can be used as a platform to search, manage and transfer data onto other storage systems.  Common use scenarios may be characterized as follows:
    <ul>
      <li>
        Register a collection (PI Lab, Project, Run, Sample or dataset)
      </li>
      <li>
        Register a single data file/object into storage archive synchronously/asynchronously
      </li>
      <li>
        Perform an update on a metadata attribute
      </li>
      <li>
        Subscribe to a known event
      </li>
      <li>
        Generate a report
      </li>
      <li>
        Update/assign permission
      </li>
      <li>
        Perform simple search functions
      </li>
      <li>
        Download a data file/object to Globus share or local directory
      </li>
    </ul>
  </li>
  <li>
    To obtain an HPC DME account, please contact the DME team at
    <a href="mailto:HPC_DME_Admin@nih.gov" rel="nofollow">
      HPC_DME_Admin@nih.gov
    </a>
    .  Additional details on the release can be found at
    <a href="https://wiki.nci.nih.gov/display/HPC/HPCS-0006" rel="nofollow">
      https://wiki.nci.nih.gov/display/HPC/HPCS-0006
    </a>
  </li>
  <li>
    For any other questions, please contact either Miles Kimbrough (
    <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
      miles.kimbrough@nih.gov
    </a>
    ) or Eric Stahlberg (
    <a href="mailto:eric.stahlberg@nih.gov" rel="nofollow">
      eric.stahlberg@nih.gov
    </a>
    )
  </li>
</ul>
<ul>
  <li>
    2017 Globus Hackathon – Globus, a cloud-based data management and transfer platform, hosted a Hackathon in January at the NIH main campus to benefit those interested in exchanging datasets across a variety of sources.  The event provided hands-on training, sample code writing, and breakout sessions to the more than 40 attendees.  As the event was very well-received, and due to increasing demand, scheduling efforts are now underway for an NIH-wide user-focused webinar to provide a high-level overview of Globus and steps to start using the service.  More details are forthcoming – in the meantime please contact Miles Kimbrough (
    <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
      miles.kimbrough@nih.gov
    </a>
    ) to discuss further.
  </li>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-1/10/17">
  1/10/17
</h3>
<ul>
  <li>
    2017 Globus Hackathon – Globus, a cloud-based data management and transfer platform, will be hosting a Hackathon at the NIH on January 24-25.  This event will benefit those interested in exchanging datasets across a variety of sources and will be separated into two training sessions based on prior technical experience – a high-level overview for researchers and system administrators, and a developer-focused session.   The Hackathon is free to NIH employees and will include hands-on training, sample code writing, and breakout sessions to promote cross-collaboration.  Register
    <a href="https://lms.learning.hhs.gov/Saba/Web/Main/goto/GuestOfferingDetails?offeringId=class000000000121278" rel="nofollow">
      here
    </a>
     and review the agenda
    <a href="http://www.globusworld.org/tour/program" rel="nofollow">
      here
    </a>
    .
  </li>
  <li>
    CBIIT TechScouts – CBIIT TechScouts currently serves the CBIIT community as an idea-sharing platform which allows members to share new tools, technologies, and methodologies relevant to added workplace efficiency.  In order to promote broader community engagement and increase awareness of this platform, the TechScouts will be undergoing some exciting new developments throughout 2017.  More updates will follow soon – in the meantime, please contact Miles Kimbrough (
    <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
      miles.kimbrough@nih.gov
    </a>
    ) to discuss further.
  </li>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-12/16/16">
  12/16/16
</h3>
<ul>
  <li>
    SuperComputing 2016 - As a follow-up to the 2016 International Conference for High Performance Computing, a blog has been made available on the NCIP Hub which summarizes NCI&#39;s involvement in the conference and sheds light on the challenges and opportunities that lay ahead.  The blog can be found at
    <a href="https://ncip.nci.nih.gov/blog/precision-medicine-inspires-hpc/" rel="nofollow">
      https://ncip.nci.nih.gov/blog/precision-medicine-inspires-hpc/
    </a>
    .
  </li>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-12/8/16">
  12/8/16
</h3>
<ul>
  <li>
    Frontiers of Predictive Oncology and Computing Meeting (FPOC)– With over 100 attendees from across the Department of Energy, the National Cancer Institute, academia, industry and other government agencies, the FPOC meeting (hosted by Intel July 12-14, 2016) provided an opportunity to gain insight into challenges and opportunities for the future. The white paper summarizing the meeting is now available through Intel’s website and can be found at
    <a href="http://www.intel.com/content/www/us/en/government/predictive-oncology-and-computing.html" rel="nofollow">
      http://www.intel.com/content/www/us/en/government/predictive-oncology-and-computing.html
    </a>
    .
  </li>
  <li>
    SuperComputing 2016 – The 2016 International Conference for High Performance Computing welcomed NCI CIO, Dr. Warren Kibbe and other representatives from CBIIT in Salt Lake City, UT, November 12-18, for a series of presentations and workshop sessions.  These included a Computational Approaches for Cancer workshop, a Plenary Session with Dr. Kibbe serving as a panelist, and a Birds-of-a-Feather session.  Electronic copies of these presentations will be circulated over the next month – more information on the conference can be found at
    <a href="http://sc16.supercomputing.org/" rel="nofollow">
      http://sc16.supercomputing.org/
    </a>
    .
  </li>
  <li>
    2017 Globus Hackathon – Globus, a cloud-based data management and transfer platform, will be hosting a Hackathon at the NIH on January 24-25.  The Hackathon is free to NIH employees and will include hands-on training, sample code writing, and breakout sessions to promote cross-collaboration.  Register
    <a href="https://lms.learning.hhs.gov/Saba/Web/Main/goto/GuestOfferingDetails?offeringId=class000000000121278" rel="nofollow">
      here
    </a>
     and review the agenda
    <a href="http://www.globusworld.org/tour/program" rel="nofollow">
      here
    </a>
    .
  </li>
  <li>
    Communications about HPC in NCI – To reflect current and emerging efforts of the NCI’s collaboration with the DOE, a landing page has been created at
    <a href="http://hpc.cancer.gov/" rel="nofollow">
      http://hpc.cancer.gov
    </a>
     which will provide an anchor point for new developments and resources related to the collaboration.
  </li>
  <li>
    HPC Education and Training – As a part of ongoing outreach efforts, The HPC Program will be coordinating a PI-level presentation with the CCR in early January.  Primary presentation will include application domains, NCI DOE collaborative updates, and Big Data management, among others.  For more information, please contact Eric Stahlberg (
    <a href="mailto:eric.stahlberg@nih.gov" rel="nofollow">
      eric.stahlberg@nih.gov
    </a>
    ) or George Zaki (
    <a href="mailto:george.zaki@nih.gov" rel="nofollow">
      george.zaki@nih.gov
    </a>
    ).
  </li>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-10/3/16">
  10/3/16
</h3>
<ul>
  <li>
    High Performance Computing (HPC) Education &amp; Training – The HPC Program will be sponsoring a Graphics Processing Unit (GPU) Training Workshop on Tuesday, October 25th, in collaboration with CIT.  The workshop will cover the fundamentals of GPU architecture and programming through hands-on exercises.  Please contact either George Zaki (
    <a href="mailto:George.zaki@nih.gov" rel="nofollow">
      George.zaki@nih.gov
    </a>
    ) or Miles Kimbrough (
    <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
      miles.kimbrough@nih.gov
    </a>
    ) for more information.
  </li>
  <li>
    2017 Globus Hackathon – Globus, a cloud-based data management and exchange platform, will be hosting a Hackathon at the NCI in late January.  The Hackathon will include hands-on training, sample code writing, and breakout sessions to promote cross-collaboration.  Stay tuned for further information and registration details.
  </li>
  <li>
    HPC and Data Management Support – For assistance with Globus, GPUs, and other high-performance computing or data management needs, visit NCI at Your Service (
    <a href="https://service.cancer.gov/" rel="nofollow">
      service.cancer.gov
    </a>
    ) and simply submit a request for support using the ‘High Performance Computing / Data Management’ request button. Or, simply email George Zaki (
    <a href="mailto:george.zaki@nih.gov" rel="nofollow">
      george.zaki@nih.gov
    </a>
    ) or Miles Kimbrough (
    <a href="mailto:miles.kimbrough@nih.gov" rel="nofollow">
      miles.kimbrough@nih.gov
    </a>
    ) to get started.
  </li>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-9/6/16">
  9/6/16
</h3>
<ul>
  <li>
    Archive Data Management – A new archive data service for large data is now available for evaluation. The new service provides application interfaces to readily access backend storage technologies such as the new Cleversafe storage moved into production last month. Individuals and groups interested in learning more about how this resource may benefit current initiatives may reach out and contact
    <a href="mailto:eric.stahlberg@nih.gov?subject=HPC%20Education%20&amp;%20Training" rel="nofollow">
      Eric Stahlberg
    </a>
    ,
    <a href="mailto:miles.kimbrough@nih.gov?subject=HPC%20Education%20&amp;%20Training" rel="nofollow">
      Miles Kimbrough
    </a>
     or
    <a href="mailto:george.zaki@nih.gov?subject=HPC%20Education%20&amp;%20Training" rel="nofollow">
      George Zaki
    </a>
    .
  </li>
  <li>
    Education and Training -  Learn more about how high-performance computing (HPC) can be used to accelerate cancer research and clinical applications. Individuals and groups interested in learning more about HPC, either in general or with specific technologies and scientific challenges in mind may reach out and contact Eric Stahlberg, Miles Kimbrough or George Zaki.
  </li>
  <li>
    The upcoming Computational Approaches for Cancer workshop scheduled for November 13, 2016 as part of the International Conference for High Performance Computing, Networking, Storage and Analysis has extended a call for extended abstracts until September 15, 2016. More information can be obtained at the link
    <a href="http://www.scworkshops.net/cancer2016/" rel="nofollow">
      http://www.scworkshops.net/cancer2016/
    </a>
    .
  </li>
</ul>
<h3 id="StrategicDataScienceInitiativesProgram-8/8/16">
  8/8/16
</h3>
<ul>
  <li>
    Frontiers of Predictive Oncology and Computing Meeting - With over 100 attendees from across the Department of Energy, NCI, academia, industry and other government agencies, the meeting (hosted by Intel July 12-14, 2016) provided an opportunity to gain insight into challenges and opportunities for the future. A white paper summarizing the meeting is to be developed.
  </li>
  <li>
    New Data Services with Cleversafe – The Cleversafe storage system officially was moved into a production operational status at the beginning of August. Led by the IT Operations Group at Frederick National Laboratory and working with many stakeholders including CCR, CBIIT and NIH CIT, the new system is used within industry and in key efforts such as the Genomic Data Commons to provide a high level of data assurance for archive and stable data. Stay tuned for further information on opportunities to learn more how this new resource may benefit your scientific and operational needs.
  </li>
  <li>
    Education and Training - Plans are underway to develop educational opportunities to learn more about how high-performance computing (HPC) can be used to accelerate cancer research and clinical applications. Individuals and groups interested in learning more about HPC, either in general or with specific technologies and scientific challenges in mind may reach out and contact
    <a href="mailto:eric.stahlberg@nih.gov?subject=HPC%20Education%20&amp;%20Training" rel="nofollow">
      Eric Stahlberg
    </a>
    ,
    <a href="mailto:miles.kimbrough@nih.gov?subject=HPC%20Education%20&amp;%20Training" rel="nofollow">
      Miles Kimbrough
    </a>
     or
    <a href="mailto:george.zaki@nih.gov?subject=HPC%20Education%20&amp;%20Training" rel="nofollow">
      George Zaki
    </a>
    .
  </li>
  <li>
    Computational Approaches for Cancer workshop - Scheduled for November 13, 2016 as part of the International Conference for High Performance Computing, Networking, Storage and Analysis. A call for papers has been issued. More information can be obtained at the link
    <a href="http://www.scworkshops.net/cancer2016/" rel="nofollow">
      http://www.scworkshops.net/cancer2016/
    </a>
  </li>
</ul>
Blog Posts
Click Here for Blog Posts
<h4>
  Blog Posts
</h4>
<ul>
  <li>
    Blog:
    <a href="/pages/viewpage.action?pageId=328336515" rel="nofollow">
      CBIIT Guest Seminar : Streamlined Transfer and Sharing of Large-scale Sensitive Data to Advance Cancer Research
    </a>
    created by
    Unknown User (kimbroughmg)

    Sep 20, 2016
    <a href="/display/HPC" rel="nofollow">
      HPC Program
    </a>
  </li>
  <li>
    Blog:
    <a href="/pages/viewpage.action?pageId=326175644" rel="nofollow">
      Explore Refactoring and Accelerating a Molecular Dynamics Application on GPUs
    </a>
    created by
    Unknown User (kimbroughmg)

    Aug 05, 2016
    <a href="/display/HPC" rel="nofollow">
      HPC Program
    </a>
  </li>
  <li>
    Blog:
    <a href="/pages/viewpage.action?pageId=326175642" rel="nofollow">
      High Performance Computing in Biomedical and Life Science Applications
    </a>
    created by
    Unknown User (kimbroughmg)

    Aug 05, 2016
    <a href="/display/HPC" rel="nofollow">
      HPC Program
    </a>
  </li>
  <li>
    Blog:
    <a href="/pages/viewpage.action?pageId=326175640" rel="nofollow">
      From MATS to MATS-NIH: 4X speedup with 4 lines of code
    </a>
    created by
    Unknown User (kimbroughmg)

    Aug 05, 2016
    <a href="/display/HPC" rel="nofollow">
      HPC Program
    </a>
  </li>
</ul>
Helpful Links
<a href="/display/HPC/HPC+Related+Links" rel="nofollow">
  search

  Helpful Links
</a>